%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 11 pt, conference]{ieeeconf}

% This command is only needed if 
% you want to use the \thanks command
\IEEEoverridecommandlockouts

% Needed to meet printer requirements.
\overrideIEEEmargins                                     

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphicx} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf Sensitivity Analysis for Neural Network Controllers}


\author{Kyle Morgenstein$^{1,2}$\\kjm3887% <-this % stops a space
% \thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{$^{1}$Aerospace Engineering,
        UT Austin
        {\tt\small kylem@utexas.edu}}%
\thanks{$^{2}$Apptronik, Inc.
        {\tt\small kylemorgenstein@apptronik.com}}%
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Parameterized controllers like neural networks have rapidly become one of the most powerful techniques to control nonlinear systems. However, their black box nature has generally prevented them from taking advantage of control theoretic techniques such as perturbation and sensitivity analysis. In this work we derive a criterion for local exponential stability that does not require knowledge of the closed loop dynamics. We then apply that criterion to a tracking controller trained with reinforcement learning to assess its performance without ever evaluating trajectories from the policy.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Reinforcement learning (RL) has become the standard control technique in robotics due to its robustness, expressivity, and ease of deployment.
Despite these attributes, RL control policies still fail catastrophically when evaluated on out of distribution (OOD) inputs.
Due to the black box nature of RL policies, safety efforts largely focus on observing potentially destabilizing changes in the action space of the policy, and triggering safe modes when risk thresholds are reached (e.g. over-current protection).
Efforts to quantify the distribution of valid inputs during training may result in more proactive runtime anomaly detection, but such efforts provide only weak guarantees of stability given the distribution shift between simulation-based training and hardware deployment.

In this work we propose a more rigorous treatment of anomaly detection using tools from nonlinear sensitivity analysis.
Treating the trained policy as an artifact, we exploit the structure of the learning-based controller to assess its sensitivity to anomalous inputs to prevent catastrophic failure at runtime.
We then approximate the closed loop dynamics via linearization, and derive a stability criterion for local exponential stability.
We demonstrate the approach to assess the stability of a double integrator tracking controller learned via reinforcement learning.

\section{SYSTEM MODELING}

Consider a nonlinear, time varying system
\begin{equation}
\dot x_t = f(x_t) + g(x_t)u_t
\end{equation}
with state $x_t\in\mathbb{R}^n$ and control signal $u_t\in\mathbb{R}^m$.
We seek to understand the sensitivity of the closed loop dynamics without assuming knowledge of $f$ or $g$.
Let $u_t = \pi(z_t)$ be the output from a neural network controller tracking a reference signal such that the error dynamics
\begin{equation}
\begin{split}
z_t &:=x_t-x^\text{ref}_t \\
\dot z_t &= F(z_t, \pi(z_t))
\end{split}
\end{equation}
has an equilibrium at $F(0,\pi(0))=0\forall t$.
The linearized closed loop system Jacobian
\begin{equation}
J_\text{cl} = \frac{\partial F}{\partial z_t}\bigg\vert_{z_t=0}.
\end{equation}

\section{SYSTEM ANALYSIS AND DESIGN}

The closed loop Jacobian can be estimated as follows:
For $k=1,...,N$ select perturbation direction $d^{(k)}\in\mathbb{R}^n$ with $||d^{(k)}||=1$ and radius $h\in\mathbb{R}$.
Then define the forward difference
\begin{equation}
% y^{(k)} := \frac{F(hd^{(k)})-F(-hd^{(k)})}{2h}
y^{(k)} := \frac{F(hd^{(k)})-F(0)}{h}
\end{equation}
with estimator $y^{(k)}=J_\text{cl}d^{(k)}+\frac{r^{(k)}}{h}$ and remainder $r^{(k)}=\frac{1}{2}(hd^{(k)})^T\mathcal{H}(hd^{(k)})=\mathcal{O}(h^2)$.
Let $Y=[y^{(1)},...,y^{(N)}]\in\mathbb{R}^{n\times N}$, $D=[d^{(1)},...,d^{(N)}]\in\mathbb{R}^{n\times N}$ and $R=[\frac{r^{(1)}}{h},...,\frac{r^{(N)}}{h}]\in\mathbb{R}^{n\times N}$.
Then, 
\begin{equation}
\begin{split}
Y&=J_\text{cl} D+R \\
\hat J_\text{cl} &= YD^T(DD^T)^{-1}
\end{split}
\end{equation}
if $DD^T$ is invertible (i.e. $\{d^{(k)}\}$ spans $\mathbb{R}^n$).
A valid choice of $\{d^{(k)}\}$ requires $N\geq n$.

To show convergence to the equilibrium, we must show that $J_\text{cl}$ is Hurwitz.
Define the estimation error 
\begin{equation}
\Delta J_\text{cl} := \hat J_\text{cl} - J_\text{cl}.
\end{equation}
From the estimator model we have
\begin{equation} \label{esterror}
\Delta J_\text{cl} = RD^T(DD^T)^{-1}
\end{equation}
with bound $||\Delta J_\text{cl}||_2\leq||R||_2||D^T(DD^T)^{-1}||_2$.
Using the singular value decomposition $D=U\Sigma V^T$, the directional term on the right-hand side can be simplified $D^T(DD^T)^{-1}=V\Sigma^{-1} U^T$, yielding
\begin{equation} \label{Dsigma}
||D^T(DD^T)^{-1}||_2=\frac{1}{\sigma_\text{min}(D)}.
\end{equation}
Where $\{d^{(k)}\}$ is selected as $N$ i.i.d unit norm isotropic random directions, $DD^T\approx \frac{N}{n}I$ results in $\sigma_\text{min}(D)\approx\sqrt{\frac{N}{n}}$ with high probability for large $N$.
Next, by assuming $F\in\mathcal{C}^2$, we can bound the second-order Taylor remainder as $||\frac{r^{(k)}}{h}||_2\leq\frac{1}{2}L_2h||d^{(k)}||_2$.
The constant $L_2$ can be estimated via the second directional derivative
\begin{equation}
\begin{split}
\hat L_2&=\sup_{||d||=1}||D^2F(0)[d,d]||_2\\
&\approx \max_k \bigg|\bigg|\frac{F(hd^{(k)})-2F(0)+F(-hd^{(k)})}{h^2}\bigg|\bigg|
\end{split}
\end{equation}
To account for numerical error, a small scale may be used $L_2=\beta\hat L_2$, $\beta> 1$.
Then,
\begin{equation} \label{Rbound}
% ||R||_2\leq\max_k||r^{(k)}||_2\sqrt{N} 
\begin{split}
||R||_2&\leq\max_k||\frac{r^{(k)}}{h}||_2\sqrt{\text{rank}(R)} \\
&\leq\max_k||\frac{r^{(k)}}{h}||_2\sqrt{\min(n,N)} \\
&\leq\frac{1}{2}L_2h\sqrt{n}
\end{split}
\end{equation}
using the requirement that $\{d^{(k)}\}$ spans $\mathbb{R}^n$.
% with factor $\sqrt{N}$ the spectral norm of a matrix whose columns are each size $\frac{1}{2}L_2h^2$.
Substituting bounds into Eq. \ref{esterror}, we find a computable bound on the estimation error
\begin{equation}
||\Delta J_\text{cl}||_2\leq\frac{1}{2}L_2h\frac{n}{\sqrt{N}}
\end{equation}

From this upper bound we may now derive the conditions to certify that $J_\text{cl}$ is Hurwitz given $\hat J_\text{cl}$.
Assume $J_\text{cl}$ is diagonalizable.
Let $\hat J_\text{cl}=V\Lambda V^{-1}$ with eigenvalues $\{\hat \lambda_i\}$ and margin $\hat \lambda = -\max_i \operatorname{Re}{\hat \lambda_i}$.
The Baur-Fike Theorem \cite{bauer1960norms} gives a bound on the distance between an eigenvalue $\lambda_i$ of $J_\text{cl}=\hat J_\text{cl} - \Delta J_\text{cl}$ and $\hat \lambda_i$:
\begin{equation}
\begin{split}
|\lambda_i-\hat \lambda_i|&\leq\kappa(V)||\Delta J_\text{cl}||_2 \\
&\leq\kappa(V)\frac{1}{2}L_2h\frac{n}{\sqrt{N}}
\end{split}
\end{equation}
with condition number $\kappa(V)=||V||_2||V^{-1}||_2$ for the matrix of eigenvectors of $\hat J_\text{cl}$.
Therefore, if 
\begin{equation} \label{datboi}
\kappa(V)\frac{1}{2}L_2h\frac{n}{\sqrt{N}}\leq\hat\lambda
\end{equation}
then
\begin{equation}
\begin{split}
\operatorname{Re}{\lambda_i}&\leq\operatorname{Re}{\hat \lambda_i}+|\lambda_i-\hat\lambda_i|\\
&\leq-\hat\lambda+\kappa(V)\frac{1}{2}L_2h\frac{n}{\sqrt{N}}\\
&<0\forall\lambda_i.
\end{split}
\end{equation}
Thus, Eq. \ref{datboi} is sufficient to conclude that $J_\text{cl}$ is Hurwitz.
We have now certified that the closed loop dynamics are locally exponentially stable based on the sampled response within the perturbation radius $h$ for each $t>t_0$.
This proof holds despite only assuming $J_\text{cl}$ is diagonalizable, $\{d^{(k)}\}$ is full rank, and $F\in\mathcal{C}^2$.

\section{SIMULATION RESULTS}

The usefulness of the above procedure is demonstrated as follows:
Let $\dot x_t = [v_t, \pi(z_t)]^T\in\mathbb{R}^4$ be a double integrator in the $X-Y$ plane controlled by an acceleration $a_t=\pi(z_t)\in\mathbb{R}^2$.
The policy $\pi_\theta$ is represented as a multivariate Gaussian with mean $\mu_\theta(x_t)$ from a  multi-layer perceptron (MLP) and covariance $\Sigma_\theta$ parameterized by $\theta=\left[\{W_i,b_i\}_0^{L-1},\Sigma\right]$.
An MLP is a nonlinear operator composition $g_i(s_i) =(\sigma_i \circ f_i)(s_i)$ of $L$ affine maps $f_i(s_i)=W_i^Ts_i+b_i$ and nonlinear differentiable activations $\sigma_i \in C^1$:
\begin{equation}
\begin{split}
s_0 &= x_t \\
s_{i+1} &= g_i(s_i) = \sigma_i(W_i^Ts_i+b_i) \\
\mu_\theta &= g_{L-1} \circ g_{L-2}\circ \dots \circ g_0 \\
\pi_\theta(u_t|x_t) &= \mathcal{N}(\mu_\theta(x_t),\Sigma_\theta)
\end{split}
\end{equation}
After training, we treat the policy as deterministic $u_t=\pi_\theta(x_t)=\mu_\theta(x_t)$ for state observation $x_t$.
We drop the weight parameterization $\theta$ for clarity.
Without loss of generality we select $x_t^\text{ref}=0\forall t>t_0$ so $z_t=x_t$.
The policy is trained via Proximal Policy Optimization \cite{ppo} to minimize the cost
\begin{equation}
C(t)=||x_t||^2_2+0.01||\dot x_t||_2^2+0.001||\ddot x_t||_2^2 
\end{equation}
The policy is trained for 1000 iterations using the \verb|IsaacLab| \cite{mittal2025isaaclab} simulation environment and algorithms adapted from \verb|RSL-RL| \cite{schwarke2025rslrl}.
To check for convergence, the value estimate from the critic is evaluated in Fig. \ref{critic_xy_synth} at 10,000 points uniformly sampled from the range $[-1.0,1.0]$.
\begin{figure}[thpb]
\centering
\includegraphics[scale=0.65]{media/critic_xy_synth.png}
\caption{Value estimates from the trained critic network.}
\label{critic_xy_synth}
\end{figure}
While the critic nearly approximates the symmetric cost function, the level sets clearly show asymmetry and irregularity in the learned value estimate.
The sensitivity can be quantified by inspecting the policy sensitivity matrix in Fig. \ref{policy_sensi_synth}.
\begin{equation}
\begin{split}
J_\pi(x_t)&=\nabla_x u_t \\
\hat C &= \mathbb{E}_{x_t \sim \rho(x_0)}\left [ J_\pi(x_t)^T J_\pi(x_t) \right ]\\
\end{split}
\end{equation}
given uniform prior density $\rho$.
\begin{figure}[thpb]
\centering
\includegraphics[scale=0.65]{media/policy_sensi_synth.png}
\caption{Policy sensitivity over uniformly sampled states.}
\label{policy_sensi_synth}
\end{figure}
Given the severe artifacts in the policy sensitivity, it is unlikely that the policy has converged to a locally exponentially stable solution.
We compare the synthetic sensitivity to the sensitivity evaluated over real trajectories. We sample 8192 initial configurations in the unit disc and allow the policy to guide the trajectory over 100 steps.
The resulting sensitivity is shown in Fig. \ref{policy_sensi_real}
\begin{figure}[thpb]
\centering
\includegraphics[scale=0.65]{media/policy_sensi_real.png}
\caption{Policy sensitivity over the policy state visitation distribution.}
\label{policy_sensi_real}
\end{figure}
Following the finite differencing scheme, the eigenvalues of the linearized closed loop dynamics are shown in Fig. \ref{eigval}.
The approximation was made over 5000 samples with radius $h=0.01$.
Because the eigenvalues have real part greater than the derived stability condition, we can conclude that the closed loop dynamics are not locally exponentially stable.
With this stability criterion, we can evaluate the behavior of the closed loop dynamics without ever having to run the policy.
This is particularly valuable when it may be dangerous to run the policy due to uncertainty or cost.

\section{CONCLUSIONS}

In this work we derived a computable criterion for local exponential stability for the closed loop dynamics of a system controlled with a neural network. 
Additionally, we showed how using the Jacobian of the learned policy and critic modules could be used to analyze the performance of the controller without having to evaluate trajectories.
In future work, we aim to extend this work to high degree of freedom systems such as a legged robot. 
When formulated as a tracking controller, the neural network has a natural equilibrium point at the origin at each time step in the reference, which provides a powerful starting point for analysis.
To combat the curse of dimensionality, we also seek to use the eigenvectors of the sensitivity matrix as a search heuristic for sampling only the most sensitive directions for the policy.
As robots rapidly enter human environments, it is vital that we ensure safety both for the robots and for the people they work with.
% Providing rigorous stability bounds will help push us closer to that goal.
Videos and additional results can be found at \verb|https://github.com/KyleM73/nonlinear_ct/|.
\begin{figure}[thpb]
\centering
\includegraphics[scale=0.4]{media/eigval.png}
\caption{Eigenvalues of the linearized closed loop dynamics.}
\label{eigval}
\end{figure}

% This command serves to balance the column lengths
% on the last page of the document manually. It shortens
% the text height of the last page by a suitable amount.
% This command does not take effect until the next page
% so it should come on the page before the last. Make
% sure that you do not shorten the text height too much.
% \addtolength{\textheight}{-12cm}  
\newpage
\nocite{*}
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

\end{document}
